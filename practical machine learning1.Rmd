---
title: "practical machine learning"
author: "tanvi"
date: "Sunday, June 22, 2014"
output: html_document
---
Practical Machine Learning Project
========================================================

I used statistics and  reasoning to reduce the number of predictors to the smallest subset that would give a strong prediction of the supplied assignment test sample.  In this case, the smallest number of predictors was the user and time aspects captured by the device

To actually get a model that generalizes to the real world, I played around some with the data and from other sources.  My tests showed that to get good prediction across devices and people (different shapes, sizes, genders, fitness levels) it needs to  the device to a Euclidian space and modeling movement with speed, direction, and shape (yaw, roll, etc).  However this seemed beyond the scope of this class project.

In this project, I estimate the out of sample error to be near 0.  Using cross validation I was getting between 99.9% and 100% accuracy and vote validation was giving me 99.9 to 100% as well. In the real world I may  call the model over fit.  However for this assignment, this accuracy is fine.  

The only data transformation I did was to turn the cvtd_timestamp column into a julian number.  The data  presents colinear predictors which may not be good.  However since the documentation on these fields is minimal, the models I chose can handle colinearaity and I am getting sparkling testing and validation accuracy, the related fields can be left alone.

Since this data is low dimensional, and I am only using user name and time based rules to predict the exercise, trees are a great approach.  From the available caret options, I like C5.0, RandomForests and GBM.  Sinc3e I am doing boosting with C5.0 and GBM,  getting 3 Random Forests classifiers, each using a different splitting algorithim.

For cross validation, since the training data CSV file is obviously ordered.  Using a k-fold cross validation may not be good unless there is some randomization of the data order first.  Otherwise the fold is just pulling related records.  Hence I use boost-632.

The code is below. 

```{r}
rm(list = ls(all = TRUE))

library(caret)

# taken from the assignment to write the files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

# a 3 way vote with tie breaker
vote <- function (tieBreakerVote, vote2, vote3) {
  if(tieBreakerVote == vote2 || tieBreakerVote == vote3) {
    majority <- tieBreakerVote
  } else if (vote2 == vote3) {
    majority <- vote2
  } else {
    majority <- tieBreakerVote
  }
  return(majority)
}

inData <- read.table('pml-training.csv', 
                     header=TRUE, 
                     as.is = TRUE, 
                     stringsAsFactors = FALSE, 
                     sep=',', 
                     na.strings=c('NA','','#DIV/0!'))

# convert the timestamp to a julian for easier comparisons
inData$cvtd_timestamp <- unclass(as.POSIXct(strptime(inData$cvtd_timestamp, 
                                                     '%d/%m/%Y  %H:%M')))

# turn any character fields (user_name is the only one) into categories
# do not turn the target (classe) into categories since we want to 
# use algorithims that can predict 1 data element
temp <- data.frame(predict(dummyVars(~ . - classe, data=inData), newdata=inData))
cleanData <- merge(temp, inData[,c('X','classe')], by='X')

# turn classe into a factor. R carest algorithims seem to like this as a factor
# I wonder if this a convience to R writers who like factors.  
cleanData$classe <- as.factor(cleanData$classe)

# just keep the predictors I am interested in
cleanData <- cleanData[,c('user_nameadelmo',
                          'user_namecarlitos',
                          'user_namecharles',
                          'user_nameeurico',
                          'user_namejeremy',
                          'user_namepedro',
                          'raw_timestamp_part_1',
                          'raw_timestamp_part_2',
                          'cvtd_timestamp',
                          'num_window', 
                          'classe')]

# tidy memory - R seems to keeps things forever
rm(temp)
rm(inData)

# split the data into training, testing and validation
# use 67% of the data as training
# use 16.5% of the data as testing
# use 16.5% of the data as validation
include <- createDataPartition(y=cleanData$classe, p=0.165, list=FALSE)
validationData <- cleanData[include,]
temp <- cleanData[-include,]

include <- createDataPartition(y = temp$classe, p=0.80, list=FALSE)
trainingData <- temp[include,]
testingData  <- temp[-include,]

rm(temp)
rm(cleanData)

# Preprocess these predictors.  
# Scaling is very important since
# these variables have very different scales.
# I found centering and transforming, YeoJohnson in this case, to be less important
# but it does not take long.
preProcessColumns <- c('raw_timestamp_part_1', 
                       'raw_timestamp_part_2',
                       'cvtd_timestamp',
                       'num_window' )

preObj <- preProcess(trainingData[,preProcessColumns],
                     method=c('YeoJohnson', 'center', 'scale'))

trainingData[preProcessColumns]   <- predict(preObj, 
                                             newdata=trainingData[preProcessColumns])
testingData[preProcessColumns]    <- predict(preObj, 
                                             newdata=testingData[preProcessColumns])
validationData[preProcessColumns] <- predict(preObj, 
                                             newdata=validationData[preProcessColumns])

# For cross validation
fitControl <- trainControl(method='boot632')

c50Grid <- expand.grid(trials = c(40,50,60,70), 
                       model = c('tree'), 
                       winnow = c(TRUE, FALSE))
modC50  <- train(classe ~ ., 
                 data = trainingData, 
                 method='C5.0', 
                 trControl=fitControl, 
                 tuneGrid = c50Grid)
predC50 <- predict (modC50, testingData)
cmC50   <- confusionMatrix (predC50, testingData$classe)

# rf seems to pick good default tuning parameters for this model
modRF  <- train(classe ~ ., 
                data = trainingData, 
                method = 'rf', 
                trControl = fitControl) 

predRF <- predict(modRF, testingData)
cmRF   <- confusionMatrix(predRF, testingData$classe)

# gbm takes a lot of memory, just use good enough parameters
gbmGrid <-  expand.grid(n.trees = c(250), 
                        shrinkage=c(0.1),
                        interaction.depth=c(5))
modGBM  <- train(classe ~ ., 
                 data=trainingData, 
                 method='gbm', 
                 trControl=fitControl, 
                 #tuneGrid = gbmGrid,
                 verbose = FALSE)
predGBM <- predict(modGBM, testingData)
cmGBM   <- confusionMatrix(predGBM, testingData$classe)

trainResults <- data.frame(ModelType=c('C50', 'RF', 'GBM'), 
                           Accuracy=c(cmC50$overall['Accuracy'],
                                      cmRF$overall['Accuracy'],
                                      cmGBM$overall['Accuracy']))

# Validate for the votes
validationResults     <- data.frame(Response=validationData$classe)
validationResults$C50 <- predict(modC50, validationData)
validationResults$RF  <- predict(modRF, validationData)
validationResults$GBM <- predict(modGBM, validationData)

# How did it do?
numCorrect <- 0
for (i in 1:dim(validationData)[1]){
  theVote <- vote(as.character(validationResults[i,'C50']),
                  as.character(validationResults[i,'RF']),
                  as.character(validationResults[i,'GBM']))
  if (theVote == validationResults[i,1]){
    numCorrect <- numCorrect + 1
  } else {    
    cat('@@@@@@@@@@@@@@@ Wrong', i, 
        as.character(validationResults[i,1]), 
        as.character(validationResults[i,2]),
        as.character(validationResults[i,3]),
        as.character(validationResults[i,4]))
  }
}

percentValidationCorrect <- numCorrect / dim(validationData)[1]

# Finally to scoring
inPredict <- read.table('pml-testing.csv', 
                        header=TRUE, 
                        as.is = TRUE, 
                        stringsAsFactors = FALSE, 
                        sep=',', 
                        na.strings=c('NA','','#DIV/0!'))

#convert
inPredict$cvtd_timestamp <- unclass(as.POSIXct(strptime(inPredict$cvtd_timestamp, 
                                                        '%d/%m/%Y  %H:%M')))

# Categories
cleanPredict <- data.frame(predict(dummyVars(~ user_name + 
                                               raw_timestamp_part_1 + 
                                               raw_timestamp_part_2 + 
                                               cvtd_timestamp + 
                                               num_window, 
                                             data=inPredict), 
                                   newdata=inPredict))

# use the same preprocess data object
cleanPredict[preProcessColumns] <- predict(preObj, 
                                           newdata=cleanPredict[preProcessColumns])
predictions     <- data.frame(C50=predict(modC50, cleanPredict))
predictions$RF  <- predict(modRF, cleanPredict)
predictions$GBM <- predict(modGBM, cleanPredict)

# vote
for (i in 1:dim(predictions)[1]){
  theVote <- vote(as.character(predictions[i,'C50']),
                  as.character(predictions[i,'RF']),
                  as.character(predictions[i,'GBM']))
  if(i == 1){
    answers <- theVote
  } else {
    answers <- c(answers, theVote)
  }
}

# write the answers
pml_write_files(answers)

```